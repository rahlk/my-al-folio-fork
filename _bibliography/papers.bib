---
---
@article{ganesh2014entropy,
  abbr={EAAI},
  title={Entropy based Binary Particle Swarm Optimization and classification for ear detection},
  author={Ganesh, Madan Ravi and Krishna, Rahul and Manikantan, K and Ramachandran, S},
  journal={Engineering Applications of Artificial Intelligence},
  volume={27},
  pages={115--128},
  year={2015},
  publisher={Pergamon}
}

@inproceedings{krishna2015actionable,
  abbr={ASE-Workshop},
  title={Actionable=Cluster+Contrast?},
  author={Krishna, Rahul and Menzies, Tim},
  booktitle={Automated Software Engineering Workshop (ASEW), 2015 30th IEEE/ACM International Conference on},
  pages={14--17},
  year={2015},
  organization={IEEE}
}

@inproceedings{krishna2016too,
  abbr={ASE},
  title={Too much automation? The bellwether effect and its implications for transfer learning},
  author={Krishna, Rahul and Menzies, Tim and Fu, Wei},
  booktitle={Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
  pages={122--131},
  year={2016}
}

@inproceedings{krishna2016bigse,
  abbr={ICSE-BigDSE},
  title={The 'BigSE' Project: Lessons Learned from Validating Industrial Text Mining},
  author={Krishna, Rahul and Yu, Zhe and Agrawal, Amritanshu and Dominguez, Manuel and Wolf, David},
  booktitle={Big Data Software Engineering (BIGDSE), 2016 IEEE/ACM 2nd International Workshop on},
  pages={65--71},
  year={2016},
  organization={IEEE}
}

@article{krishna2018bellwethers,
  abbr={TSE},
  title={Bellwethers: A Baseline Method For Transfer Learning},
  author={Krishna, Rahul and Menzies, Tim},
  journal={IEEE Transactions on Software Engineering},
  year={2018}
}

@article{krishna2017less,
  abbr={IST},
  title={Less is more: Minimizing code reorganization using XTREE},
  author={Krishna, Rahul and Menzies, Tim and Layman, Lucas},
  journal={Information and Software Technology},
  volume={88},
  pages={53--66},
  year={2017},
  publisher={Elsevier}
}

@article{chen2018sampling,
  abbr={TSE},
  title={"Sampling" as a Baseline Optimizer for Search-based Software Engineering},
  author={Chen, Jianfeng and Nair, Vivek and Krishna, Rahul and Menzies, Tim},
  journal={IEEE Transactions on Software Engineering},
  year={2018},
  publisher={IEEE}
}

@inproceedings{chen2018applications,
  abbr={FSE}, 
  title={Applications of psychological science for actionable analytics},
  author={Chen, Di and Fu, Wei and Krishna, Rahul and Menzies, Tim},
  booktitle={2018 ACM 26th Symposium on the Foundations of Software Engineering (ECSE/FSE)},
  pages={456--467},
  year={2018}
}

@article{xia2018hyperparameter,
  abbr={arXiv},
  title={Hyperparameter optimization for effort estimation},
  author={Xia, Tianpei and Krishna, Rahul and Chen, Jianfeng and Mathew, George and Shen, Xipeng and Menzies, Tim},
  journal={arXiv preprint arXiv:1805.00336},
  year={2018}
}

@inproceedings{agrawal2018we,
  abbr={ICSE-SEIP},
  title={We don't need another hero? The impact of heroes on software development},
  author={Agrawal, Amritanshu and Rahman, Akond and Krishna, Rahul and Sobran, Alexander and Menzies, Tim},
  booktitle={Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice},
  pages={245--253},
  year={2018},
  organization={ACM}
}

@inproceedings{krishna2018connection,
  abbr={ICSE-SEIP},
  title={What is the connection between issues, bugs, and enhancements? Lessons learned from 800+ software projects},
  author={Krishna, Rahul and Agrawal, Amritanshu and Rahman, Akond and Sobran, Alexander and Menzies, Tim},
  booktitle={Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice},
  pages={306--315},
  year={2018},
  organization={ACM},
  html={https://dl.acm.org/doi/10.1145/3183519.3183548},
  pdf={https://arxiv.org/pdf/1710.08736.pdf}
}

@inproceedings{rahman2018characterizing,
  abbr={SWAN},
  title={Characterizing the influence of continuous integration: Empirical results from 250+ open source and proprietary projects},
  author={Rahman, Akond and Agrawal, Amritanshu and Krishna, Rahul and Sobran, Alexander},
  booktitle={Proceedings of the 4th ACM SIGSOFT International Workshop on Software Analytics},
  pages={8--14},
  year={2018},
  html={https://dl.acm.org/doi/abs/10.1145/3278142.3278149},
  pdf={https://arxiv.org/pdf/1711.03933.pdf}
}

@inproceedings{wang2019isense,
  abbr={ICSE üèÜ},
  title={iSENSE: Completion-aware crowdtesting management },
  author={Wang, Junjie and Yang, Ye and Krishna, Rahul and Menzies, Tim and Wang, Qing},
  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},
  pages={912--923},
  year={2019},
  organization={IEEE},
  html={https://2019.icse-conferences.org/details/icse-2019-Technical-Papers/32/iSENSE-Completion-Aware-Crowdtesting-Management},
  pdf={/assets/pdf/iSENSE_icse19.pdf}
}

@inproceedings{iqbal2022unicorn,
      abbr={EuroSys},
      selected={true},
      title={Unicorn: Reasoning about Configurable System Performance through the lens of Causality}, 
      abstract={Modern computer systems are highly configurable, with the variability space sometimes larger than the number of atoms in the universe. Understanding and reasoning about the performance behavior of highly configurable systems, due to a vast variability space, is challenging. State-of-the-art methods for performance modeling and analyses rely on predictive machine learning models, therefore, they become (i) unreliable in unseen environments (e.g., different hardware, workloads), and (ii) produce incorrect explanations. To this end, we propose a new method, called Unicorn, which (a) captures intricate interactions between configuration options across the software-hardware stack and (b) describes how such interactions impact performance variations via causal inference. We evaluated Unicorn on six highly configurable systems, including three on-device machine learning systems, a video encoder, a database management system, and a data analytics pipeline. The experimental results indicate that Unicorn outperforms state-of-the-art performance optimization and debugging methods. Furthermore, unlike the existing methods, the learned causal performance models reliably predict performance for new environments.},
      author={Iqbal, Md Shahriar and Krishna, Rahul and Javidian, Mohammad Ali and Ray, Baishakhi and Jamshidi, Pooyan},
      booktitle={The European Conference on Computer Systems (EuroSys).},
      year={2022},
      html={https://arxiv.org/abs/2201.08413},
      pdf={https://arxiv.org/pdf/2201.08413.pdf},
}

@inproceedings{she2020mtfuzz,
  abbr={FSE},
  selected={true},
  title={MTFuzz: Fuzzing with a Multi-Task Neural Network},
  abstract={Fuzzing is a widely used technique for detecting software bugs and vulnerabilities. Most popular fuzzers generate new inputs using an evolutionary search to maximize code coverage. Essentially, these fuzzers start with a set of seed inputs, mutate them to generate new inputs, and identify the promising inputs using an evolutionary fitness function for further mutation. Despite their success, evolutionary fuzzers tend to get stuck in long sequences of unproductive mutations. In recent years, machine learning (ML) based mutation strategies have reported promising results. However, the existing ML-based fuzzers are limited by the lack of quality and diversity of the training data. As the input space of the target programs is high dimensional and sparse, it is prohibitively expensive to collect many diverse samples demonstrating successful and unsuccessful mutations to train the model. In this paper, we address these issues by using a Multi-Task Neural Network that can learn a compact embedding of the input space based on diverse training samples for multiple related tasks (i.e., predicting for different types of coverage). The compact embedding can guide the mutation process by focusing most of the mutations on the parts of the embedding where the gradient is high. MTFuzz uncovers 11 previously unseen bugs and achieves an average of 2√ó more edge coverage compared with 5 state-of-the-art fuzzer on 10 real-world programs.},
  author={She, Dongdong and Krishna, Rahul and Yan, Lu and Jana, Suman and Ray, Baishakhi},
  booktitle={2020 ACM 28th Symposium on the Foundations of Software Engineering (ECSE/FSE)},
  year={2020},
  html={https://arxiv.org/abs/2005.12392},
  pdf={https://arxiv.org/pdf/2005.12392},
}

@article{chakraborty2021deep,
  abbr={TSE},
  selected={true},
  abstract={Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning (DL) has resulted in a surge of interest in applying DL for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95% at detecting vulnerabilities. In this paper, we ask, "how well do the state-of-the-art DL-based techniques perform in a real-world vulnerability prediction scenario?". To our surprise, we find that their performance drops by more than 50%. A systematic investigation of what causes such precipitous performance drop reveals that existing DL-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baseline: up to 33.57% boost in precision and 128.38% boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing DL-based vulnerability prediction systems' potential issues and draws a roadmap for future DL-based vulnerability prediction research. In that spirit, we make available all the artifacts supporting our results: https://github.com/VulDetProject/ReVeal},
  title={Deep learning based vulnerability detection: Are we there yet?},
  author={Chakraborty, Saikat and Krishna, Rahul and Ding, Yangruibo and Ray, Baishakhi},
  journal={IEEE Transactions on Software Engineering.},
  year={2021},
  publisher={IEEE},
  html={https://arxiv.org/abs/2009.07235},
  pdf={https://arxiv.org/pdf/2009.07235},
}

@article{krishna2020conex,
  abbr={TSE},
  selected={true},
  abstract={Configuration space complexity makes the big-data software systems hard to configure well. Consider Hadoop, with over nine hundred parameters, developers often just use the default configurations provided with Hadoop distributions. The opportunity costs in lost performance are significant. Popular learning-based approaches to auto-tune software does not scale well for big-data systems because of the high cost of collecting training data. We present a new method based on a combination of Evolutionary Markov Chain Monte Carlo (EMCMC) sampling and cost reduction techniques to cost-effectively find better-performing configurations for big data systems. For cost reduction, we developed and experimentally tested and validated two approaches: using scaled-up big data jobs as proxies for the objective function for larger jobs and using a dynamic job similarity measure to infer that results obtained for one kind of big data problem will work well for similar problems. Our experimental results suggest that our approach promises to significantly improve the performance of big data systems and that it outperforms competing approaches based on random sampling, basic genetic algorithms (GA), and predictive model learning. Our experimental results support the conclusion that our approach has strongly demonstrated potential to significantly and cost-effectively improve the performance of big data systems.},
  title={ConEx: Efficient exploration of big-data system configurations for better performance},
  author={Krishna, Rahul and Tang, Chong and Sullivan, Kevin and Ray, Baishakhi},
  journal={IEEE Transactions on Software Engineering},
  year={2020},
  publisher={IEEE},
  html={https://arxiv.org/abs/1910.09644},
  pdf={https://arxiv.org/pdf/1910.09644},
}

@article{krishna2020whence,
  abbr={TSE},
  selected={true},
  abstract={As software systems grow in complexity and the space of possible configurations increases exponentially, finding the near-optimal configuration of a software system becomes challenging. Recent approaches address this challenge by learning performance models based on a sample set of configurations. However, collecting enough sample configurations can be very expensive since each such sample requires configuring, compiling, and executing the entire system using a complex test suite. When learning on new data is too expensive, it is possible to use \textit{Transfer Learning} to "transfer" old lessons to the new context. Traditional transfer learning has a number of challenges, specifically, (a) learning from excessive data takes excessive time, and (b) the performance of the models built via transfer can deteriorate as a result of learning from a poor source. To resolve these problems, we propose a novel transfer learning framework called BEETLE, which is a "bellwether"-based transfer learner that focuses on identifying and learning from the most relevant source from amongst the old data. This paper evaluates BEETLE with 57 different software configuration problems based on five software systems (a video encoder, an SAT solver, a SQL database, a high-performance C-compiler, and a streaming data analytics tool). In each of these cases, BEETLE found configurations that are as good as or better than those found by other state-of-the-art transfer learners while requiring only a fraction (1/7th) of the measurements needed by those other methods. Based on these results, we say that BEETLE is a new high-water mark in optimally configuring software.},
  title={Whence to learn? transferring knowledge in configurable systems using beetle},
  author={Krishna, Rahul and Nair, Vivek and Jamshidi, Pooyan and Menzies, Tim},
  journal={IEEE Transactions on Software Engineering},
  year={2020},
  publisher={IEEE},
  html={https://arxiv.org/abs/1911.01817},
  pdf={https://arxiv.org/pdf/1911.01817},
}

@article{krishna2020cadet,
  abbr={NeurIPS},
  abstract={Modern computing platforms are highly-configurable with thousands of interacting configurations. However, configuring these systems is challenging. Erroneous configurations can cause unexpected non-functional faults. This paper proposes CADET (short for Causal Debugging Toolkit) that enables users to identify, explain, and fix the root cause of non-functional faults early and in a principled fashion. CADET builds a causal model by observing the performance of the system under different configurations. Then, it uses casual path extraction followed by counterfactual reasoning over the causal model to: (a) identify the root causes of non-functional faults, (b) estimate the effects of various configurable parameters on the performance objective(s), and (c) prescribe candidate repairs to the relevant configuration options to fix the non-functional fault. We evaluated CADET on 5 highly-configurable systems deployed on 3 NVIDIA Jetson systems-on-chip. We compare CADET with state-of-the-art configuration optimization and ML-based debugging approaches. The experimental results indicate that CADET can find effective repairs for faults in multiple non-functional properties with (at most) 17% more accuracy, 28% higher gain, and 40√ó speed-up than other ML-based performance debugging methods. Compared to multi-objective optimization approaches, CADET can find fixes (at most) 9√ó faster with comparable or better performance gain. Our case study of non-functional faults reported in NVIDIA's forum show that CADET can find 14 better repairs than the experts' advice in less than 30 minutes.},
  title={CADET: A systematic method for debugging misconfigurations using counterfactual reasoning},
  author={Krishna, Rahul and Shahriar Iqbal, Md and Javidian, Mohammad Ali and Ray, Baishakhi and Jamshidi, Pooyan},
  journal={NeurIPS 2020 (Workshop on ML for Systems).},
  year={2020},
  html={https://arxiv.org/abs/2010.06061},
  pdf={https://arxiv.org/pdf/2010.06061.pdf},
}

@article{krishna2020learning,
  abbr={EMSE},
  title={Learning actionable analytics from multiple software projects},
  author={Krishna, Rahul and Menzies, Tim},
  journal={Empirical Software Engineering},
  volume={25},
  number={5},
  pages={3468--3500},
  year={2020},
  publisher={Springer},
  html={https://arxiv.org/abs/1708.05442},
  pdf={https://arxiv.org/pdf/1708.05442.pdf}
}

@article{yedida2021partitioning,
  abbr={arXiv},
  title={Partitioning Cloud-based Microservices (via Deep Learning)},
  author={Yedida, Rahul and Krishna, Rahul and Kalia, Anup and Menzies, Tim and Xiao, Jin and Vukovic, Maja},
  journal={arXiv preprint.},
  year={2021},
  html={https://arxiv.org/abs/2109.14569},
  pdf={https://arxiv.org/pdf/2109.14569.pdf}
}

@inproceedings{yedida2021lessons,
  abbr={ASE},
  title={Lessons learned from hyper-parameter tuning for microservice candidate identification},
  author={Yedida, Rahul and Krishna, Rahul and Kalia, Anup and Menzies, Tim and Xiao, Jin and Vukovic, Maja},
  booktitle={The 36th IEEE/ACM International Conference on Automated Software Engineering, Industry Showcase.},
  year={2021},
  html={https://arxiv.org/abs/2106.06652},
  pdf={https://arxiv.org/pdf/2106.06652}
}

@inproceedings{kalia2021mono2micro,
  abbr={FSE},
  selected={true},
  abstract={In migrating production workloads to cloud, enterprises often face the daunting task of evolving monolithic applications toward a microservice architecture. At IBM, we developed a tool called Mono2Micro to assist with this challenging task. Mono2Micro performs spatio-temporal decomposition, leveraging well-defined business use cases and runtime call relations to create functionally cohesive partitioning of application classes. Our preliminary evaluation of Mono2Micro showed promising results. How well does Mono2Micro perform against other decomposition techniques, and how do practitioners perceive the tool? This paper describes the technical foundations of Mono2Micro and presents results to answer these two questions. To answer the first question, we evaluated Mono2Micro against four existing techniques on a set of open-source and proprietary Java applications and using different metrics to assess the quality of decomposition and tool's efficiency. Our results show that Mono2Micro significantly outperforms state-of-the-art baselines in specific metrics well-defined for the problem domain. To answer the second question, we conducted a survey of twenty-one practitioners in various industry roles who have used Mono2Micro. This study highlights several benefits of the tool, interesting practitioner perceptions, and scope for further improvements. Overall, these results show that Mono2Micro can provide a valuable aid to practitioners in creating functionally cohesive and explainable microservice decompositions.},
  title={Mono2Micro: A practical and effective tool for decomposing monolithic Java applications to microservices},
  author={Kalia, Anup K and Xiao, Jin and Krishna, Rahul and Sinha, Saurabh and Vukovic, Maja and Banerjee, Debasish},
  booktitle={Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering.},
  pages={1214--1224},
  year={2021},
  html={https://arxiv.org/abs/2107.09698},
  pdf={https://arxiv.org/pdf/2107.09698}
}